# Databricks notebook source
# MAGIC %md
# MAGIC ### Example Exploratory Notebook
# MAGIC
# MAGIC Use this notebook to explore the data generated by the pipeline in your preferred programming language.
# MAGIC
# MAGIC **Note**: This notebook is not executed as part of the pipeline.

# COMMAND ----------

spark.sql("USE CATALOG `workspace`")
spark.sql("USE SCHEMA `project`")

display(spark.sql("SELECT * FROM samples.nyctaxi.trips"))

# COMMAND ----------

# MAGIC %pip install great_expectations

# COMMAND ----------

# MAGIC %restart_python

# COMMAND ----------

import great_expectations as gx

# Create an in-memory temporary context (no folder structure)
context = gx.get_context()

print("GX context created successfully")


# COMMAND ----------

context = gx.get_context(
    mode="file",
    project_root_dir="/tmp/gx"
)

# COMMAND ----------

from pyspark.sql import SparkSession
import pandas as pd

spark = SparkSession.builder.getOrCreate()

data = [
    {"name": "Alice", "age": 25},
    {"name": "Bob", "age": 30},
    {"name": "Charlie", "age": 35},
    {"name": None, "age": 40},
]
spark_df = spark.createDataFrame(data)
spark_df.show()


# COMMAND ----------

df = spark_df.toPandas()

# COMMAND ----------

# Create a validator from Pandas
validator = context.sources.pandas_default.read_dataframe(df)

# Define expectations
validator.expect_column_values_to_not_be_null("name")
validator.expect_column_values_to_be_between("age", min_value=20, max_value=40)

# Run validation
results = validator.validate()

print(results)


# COMMAND ----------

# MAGIC %md
# MAGIC Breakpoint
# MAGIC

# COMMAND ----------

context = gx.get_context(
    mode="filesystem",
    project_root_dir="/dbfs/FileStore/gx_project"
)

# COMMAND ----------


import great_expectations as gx
import pandas as pd
from pyspark.sql import SparkSession

# Create sample Spark DataFrame
spark = SparkSession.builder.getOrCreate()
data = [
    {"name": "Alice", "age": 25},
    {"name": "Bob", "age": 30},
    {"name": "Charlie", "age": 35},
    {"name": None, "age": 40},
]
spark_df = spark.createDataFrame(data)

# Convert Spark â†’ Pandas for Great Expectations
df = spark_df.toPandas()

# ---------------------------------------------------------
# ðŸ§© STEP 1: Create a persistent GX context in DBFS
# ---------------------------------------------------------
context = gx.get_context(
    mode="file",
    project_root_dir="/dbfs/FileStore/gx_project"
)
print("âœ… Great Expectations context created successfully.")

# ---------------------------------------------------------
# ðŸ§¾ STEP 2: Create or update expectation suite
# ---------------------------------------------------------
suite_name = "demo_suite"
suite = context.add_or_update_expectation_suite(suite_name)

# ---------------------------------------------------------
# ðŸ§ª STEP 3: Create validator and define expectations
# ---------------------------------------------------------
validator = context.get_validator(
    batch_data=df,
    expectation_suite=suite
)

# Define expectations
validator.expect_column_values_to_not_be_null("name")
validator.expect_column_values_to_be_between("age", min_value=20, max_value=40)

# ---------------------------------------------------------
# ðŸ§® STEP 4: Run validation and show results
# ---------------------------------------------------------
results = validator.validate()
print("\nâœ… Validation Results:")
print(results)

# ---------------------------------------------------------
# ðŸ“‚ STEP 5: (Optional) Save results and open Data Docs
# ---------------------------------------------------------
context.build_data_docs()
# To open Data Docs (in Databricks, copy and open this path in a browser)
print("\nðŸ“Š Data Docs saved at: /dbfs/FileStore/gx_project/great_expectations/uncommitted/data_docs/local_site/index.html")

