{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7a09a14-38ca-4d47-bf7e-66c859ee8902",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **Load File**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0fd9dbd-d696-4432-879f-98655ce540e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a492bde1-1506-499e-b499-aabc5d7fe26b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Flight CSV Load\n",
    "flight_df = spark.table(\"workspace.default.flight_data\")\n",
    "flight_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5177a320-d4c2-4e08-ab15-f7a7d1493a6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## # **JSON File Reading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebb01f8a-447a-4ee5-97b9-521d44648742",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#line_delimited_json:\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# File path in your volume\n",
    "json_path = \"/Volumes/workspace/default/datsetvolume/line_delimited_json.json\"\n",
    "\n",
    "# Read line-delimited JSON\n",
    "df = spark.read.json(json_path)\n",
    "\n",
    "# Show first few rows\n",
    "df.show(truncate=False)\n",
    "\n",
    "# Print schema\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4066b22e-f84d-447e-b16b-e3efed0e510c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Multi_line_correct: \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# File path in your volume\n",
    "json_path = \"/Volumes/workspace/default/datsetvolume/Multi_line_correct.json\"\n",
    "\n",
    "# Path to store corrupt records\n",
    "bad_records_path = \"/Volumes/workspace/default/datsetvolume/bad_records\"\n",
    "\n",
    "# Read line-delimited JSON with badRecordsPath option\n",
    "df = spark.read.option(\"badRecordsPath\", bad_records_path).json(json_path)\n",
    "\n",
    "# Show first few rows\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52ce7030-a253-40c0-9da9-c32dbe961e32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Multi_line_incorrect: \n",
    "\n",
    "json_path = \"/Workspace/Users/your_username/employees_array.json\"\n",
    "\n",
    "df = spark.read.option(\"multiline\", \"true\").json(json_path)\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a1e4a8a-289c-4471-9d08-946b7637e589",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#corrupted_json: \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# File path in your volume\n",
    "json_path = \"/Volumes/workspace/default/datsetvolume/corrupted_json.json\"\n",
    "\n",
    "# Read line-delimited JSON\n",
    "df = spark.read.json(json_path)\n",
    "\n",
    "# Show first few rows\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca506c92-8ed0-42e0-a2c4-7e8341394714",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#single_file_json with extra fields: \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# File path in your volume\n",
    "json_path = \"/Volumes/workspace/default/datsetvolume/single_file_json with extra fields.json\"\n",
    "\n",
    "# Read line-delimited JSON\n",
    "df = spark.read.json(json_path)\n",
    "\n",
    "# Show first few rows\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cd00076-2d86-492a-a277-b823d8fda82a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read dbfs:/databricks-datasets/retail-org/\n",
    "display(dbutils.fs.ls(\"dbfs:/databricks-datasets/retail-org/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3414e723-3053-47dd-885d-693478aeb9f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the CSV file into a Spark DataFrame\n",
    "df = spark.read.csv(\n",
    "    \"/databricks-datasets/retail-org/customers/customers.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# Filter the DataFrame for records where the state is \"CA\"\n",
    "df_f = df.filter(df.state == \"CA\")\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "display(df_f)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1. Load File",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
